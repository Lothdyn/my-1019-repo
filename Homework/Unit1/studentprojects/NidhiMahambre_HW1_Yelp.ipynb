{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 1 Homework:  Scraping the Yelp Website\n",
    "\n",
    "Welcome!  For this homework assignment you'll be tasked with building a web scraper in a manner that builds on what was covered in our web scraping class.\n",
    "\n",
    "The assignment will extend the lab work done during that time, where we built a dataset that listed the name, number of reviews and price range for restaurant on the following web page: https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&ns=1\n",
    "\n",
    "**What You'll Turn In:**\n",
    "\n",
    "A finished jupyter notebook that walks us through the steps you took in order to get your results.  Provide notes where appropriate to explain what you are doing.\n",
    "\n",
    "The notebook should produce a finished dataset at the end.  \n",
    "\n",
    "If for some reason you're experiencing problems with the final result, please let someone know when turning it in.\n",
    " \n",
    "Your homework will be divided into three tiers, each of which have increasing levels of difficulty:\n",
    "\n",
    "##### Tier 1: Five Columns From the First Page\n",
    "\n",
    "At the most basic level for this assignment, you will need to extend what we did in class, and create a dataset that has five columns in it that are 30 rows long.  This means you will not need to go off the first page in order to complete this section.\n",
    "\n",
    "##### Tier 2:  100 Row Dataset With At Least 3 Columns\n",
    "\n",
    "For this portion of the assignment, take 3 of your columns from step 1, and extend them out to multiple pages on the yelp website.  You should appropriately account for the presence of missing values.\n",
    "\n",
    "##### Tier 3:  100 Row Dataset With At Least 5 Columns\n",
    "\n",
    "Very similar to Tier 2, but if you use this many columns you will be forced to encounter some columns that will frequently have missing values, whereas with Tier 2 you could likely skip these if you wanted to.  \n",
    "\n",
    "##### Tier 4:  100 Row Dataset With At Least 5 Columns + Individual Restaurant Categories\n",
    "\n",
    "Restaurants often have different categories associated with them, so grabbing them individually as separate values is often challenging.  To complete this tier, you'll have to find a way to 'pick out' each of the individual categories as their own separate column value.  \n",
    "\n",
    "##### Tier 5:  Unlimited Row Dataset With At Least 5 Columns + Individual Restaurant Categories\n",
    "\n",
    "Take what you did in Tier 4, and extend it so that the code will work with an arbitrary number of pages.  Ie, regardless of how many pages there are listing the best restaurants in London, your scraper will find them, and cleanly parse their information into clean datasets.\n",
    "\n",
    "### Hints\n",
    "\n",
    "Here are a few tips that will save you time when completing this assignment:\n",
    "\n",
    " - The name, average rating, total ratings and neighborhood of a restaurant tend to be the 'easy' ones, because they rarely have missing values, so what ever logic you use on the first page will typically apply to all pages.  They are a good place to start\n",
    " - Phone numbers, price ranges and reviews are more commonly missing, so if you are trying to get a larger number of items from them across multiple pages you should expect to do some error handling\n",
    " - You can specify any sort of selector when using the `find_all()` method, not just `class`.  For example, imagine you have the following `<div>` tag:\n",
    "    `<div class='main-container red-blue-green' role='front-unit' aria-select='left-below'>Some content here</div>`\n",
    "    \n",
    "   This means that when you use `scraper.find_all('div')`, you can pass in arguments like `scraper.find_all('div', {'role': 'front-unit'})` or anything else that allows you to isolate that particular tag.\n",
    " - When specifying selectors like `{'class': 'dkght__384Ko'}`, sometimes less is more.  If you include multiple selectors, you are saying return a tag with **any one of these** distinctions, not all of them.  So if your results are large, try different combinations of selectors to get the smallest results possible.\n",
    " - If you begin dealing with values that are unreliably entered, you should use the 'outside in' technique where you grab a parent container that holds the element and find a way to check to see if a particular value is there by scraping it further.  The best way to do this is to try and find a unique container for every single restaurant.  This means that you will have a reliable parent element for every single restaurant, and within *each of these* you can search for `<p>`, `<a>`, `<div>`, and `<span>` tags and apply further logic.\n",
    " - When you get results from `BeautifulSoup`, you will be given data that's denoted as either `bs4.element.Tag` or `bs4.element.ResultSet`.  They are **not the same**.  Critically, you can search a `bs4.element.Tag` for further items, but you cannot do this with a `bs4.element.ResultSet`.  \n",
    " \n",
    "   For example, let's say you grab all of the divs from a page with `scraper.find_all('div')` and save it as the variable `total_divs`.  This means `total_divs` will look somethig like this:  \n",
    "   \n",
    "   `[<div><p>Div content</p><p>Second paragraph</p></div>,`\n",
    "      `<div><p>Div content</p><p>Second paragraph</p></div>,`\n",
    "      `<div><p>Div content</p><p>Second paragraph</p></div>]`\n",
    "      \n",
    "   In this case the variable `total_divs` is a result set and there's nothing else you can do to it directly.  However, every item within `total_divs` is a tag, which means you can scrape it further.  \n",
    "   \n",
    "   So if you wanted you could write a line like:  `total_paragraphs = [div.find_all('p') for div in total_divs]`, and get the collection of paragraphs within each div.  \n",
    "   \n",
    "   If you confuse the two you'll get the following error message:  \n",
    "   \n",
    "   `AttributeError: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?`\n",
    " - The values of the different selectors change periodically on yelp, so if your scraper all of a sudden stops working that's probably why.  Ie, if you have a command like `scraper.find_all('div', {'class': '485dk0W__container09'}` that no longer returns results, the class `485dk0W__container09` may now be `r56kW__container14` or something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I attempted all 5 tiers at once, so the steps are combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Pulling data from all pages on the website\n",
    "# using a while loop to pull all restaurant containers as lists. If the page is blank, the list is null and the pull stops.\n",
    "\n",
    "\n",
    "# Counter for page number on the url- starts at 0 and increments of 10\n",
    "item_counter=0\n",
    "test_list=['start']\n",
    "\n",
    "# these are the lists that the cleaned data will go in\n",
    "restaurant_data_all=[]\n",
    "address_data_all=[]\n",
    "\n",
    "# Main container for each restaurant.This is a <div> class. It contains the <div> and <p> classes that we need\n",
    "ex_classes={'class': 'container__09f24__21w3G'}\n",
    "\n",
    "# within the main container, these are the <span> class items that we need, for restaurant data\n",
    "test_classes={\n",
    "    'class':'text__09f24__2tZKC',\n",
    "    'class':'text-color--black-extra-light__09f24__38DtK',\n",
    "    'class':'text-align--left__09f24__3Drs0' #restaurant name, price, reviews and tags\n",
    "    }\n",
    "\n",
    "# within the main container, these are the <p> class items that we need, for address data\n",
    "aa_test={'class':'text-size--small__09f24__1Z_UI' #phone, address and area, p type\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "while len(test_list)>0:\n",
    "    raw_data=requests.get(f'https://www.yelp.com/search?find_desc=Restaurants&find_loc=London%2C%20United%20Kingdom&start={item_counter}')\n",
    "    scraper=bs4.BeautifulSoup(raw_data.text)\n",
    "    \n",
    "    # get the data in the main container, in list form\n",
    "    yex_data=scraper.find_all('div',ex_classes)\n",
    "    aa=[data.find_all(['p','div']) for data in yex_data]\n",
    "    \n",
    "    # this is the while-loop test: if this is null the pull stops\n",
    "    test_list=aa\n",
    "    \n",
    "    for item in aa:\n",
    "        restaurant_data=[]\n",
    "        address_data=[]\n",
    "        el=item[0]\n",
    "        bb=el.find_all('span',test_classes)\n",
    "        for element in bb:\n",
    "            restaurant_data.append(element.text)          \n",
    "        #print(restaurant_data)\n",
    "        cc=el.find_all('p',aa_test)\n",
    "        for piece in cc:\n",
    "            #print(piece.text)\n",
    "            address_data.append(piece.text)\n",
    "        #print(address_data) \n",
    "        restaurant_data_all.append(restaurant_data)\n",
    "        address_data_all.append(address_data)\n",
    "    \n",
    "    item_counter=item_counter+10 \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_restaurant_data:  240\n",
      "size_address_data:  240\n"
     ]
    }
   ],
   "source": [
    "#checking for completeness of data\n",
    "print(\"size_restaurant_data: \",len(restaurant_data_all))\n",
    "print(\"size_address_data: \",len(address_data_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have two lists of nested list data:\n",
    "# 1. restaurant details like name, price, reviews and tags\n",
    "# 2. address details- phone, address, location\n",
    "\n",
    "# Next step: break out the data into 'f_' lists to go into the final dataframe\n",
    "# Each item in the restaurant list corresponds to a single restaurant. An empty list is created for expected datapoints.\n",
    "#     if the expected datapoint doesnt exist,the final list will contain an empty nested list there\n",
    "#     tags data can have multiple string values, so that list may contain multiple elements\n",
    "\n",
    "f_names_list_all=[]\n",
    "f_reviews_list_all=[]\n",
    "f_price_list_all=[]\n",
    "f_tags_list_all=[]\n",
    "\n",
    "for item in restaurant_data_all:\n",
    "    #print(item)\n",
    "    f_names_list=[]\n",
    "    f_reviews_list=[]\n",
    "    f_price_list=[]\n",
    "    f_tags_list=[]\n",
    "    for element in item:\n",
    "        if element[0].isdigit() and '\\xa0' in element:\n",
    "            f_name=' '\n",
    "            f_name=f_name.join(element.split()[1:])\n",
    "            f_names_list.append(f_name) \n",
    "            \n",
    "            \n",
    "        elif element.isdigit():\n",
    "            f_reviews=element\n",
    "            f_reviews_list.append(f_reviews)\n",
    "        elif '\\xa3' in element:\n",
    "            f_price=element\n",
    "            f_price_list.append(f_price)\n",
    "        elif '\\xa0' not in element:\n",
    "            f_tag=element.replace(', ','')\n",
    "            f_tags_list.append(f_tag)\n",
    "            \n",
    "    f_names_list_all.append(f_names_list)   \n",
    "    f_reviews_list_all.append(f_reviews_list)\n",
    "    f_price_list_all.append(f_price_list)\n",
    "    f_tags_list_all.append(f_tags_list)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_names_data:  240\n",
      "size_reviewscount_data:  240\n",
      "size_pricerange_data:  240\n",
      "size_tags_data:  240\n"
     ]
    }
   ],
   "source": [
    "#checking for completeness of final dataset:\n",
    "print(\"size_names_data: \", len(f_names_list_all))\n",
    "print(\"size_reviewscount_data: \", len(f_reviews_list_all))\n",
    "print(\"size_pricerange_data: \", len(f_price_list_all))\n",
    "print(\"size_tags_data: \", len(f_tags_list_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarly, for address data\n",
    "f_phone_list_all=[]\n",
    "f_address_list_all=[]\n",
    "f_location_list_all=[]\n",
    "\n",
    "for item in address_data_all:\n",
    "    #print(item)\n",
    "    f_phone_list=[]\n",
    "    f_address_list=[]\n",
    "    f_location_list=[]\n",
    "    \n",
    "    for element in item:\n",
    "        if element[0:3]=='020':\n",
    "            f_phone_list.append(element)\n",
    "        elif element[0].isdigit():\n",
    "            f_address_list.append(element)\n",
    "        else:\n",
    "            f_location_list.append(element)\n",
    "            \n",
    "    f_phone_list_all.append(f_phone_list)   \n",
    "    f_address_list_all.append(f_address_list)\n",
    "    f_location_list_all.append(f_location_list)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_phone_data:  240\n",
      "size_address_data:  240\n",
      "size_location_data:  240\n"
     ]
    }
   ],
   "source": [
    "#testing completeness of dataset\n",
    "print('size_phone_data: ',len(f_phone_list_all))\n",
    "print('size_address_data: ',len(f_address_list_all))\n",
    "print('size_location_data: ',len(f_location_list_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is now in a list of lists. Next Step: create some new lists with values\n",
    "\n",
    "f_names_all=[]\n",
    "for item in f_names_list_all:\n",
    "    if item==[]:\n",
    "        f_names_all.append('NA')\n",
    "    else:\n",
    "        f_names_all.append(item[0])\n",
    "\n",
    "\n",
    "f_price_all=[]\n",
    "for item in f_price_list_all:\n",
    "    if item==[]:\n",
    "        f_price_all.append('NA')\n",
    "    else:\n",
    "        f_price_all.append(item[0])\n",
    "\n",
    "\n",
    "f_reviews_all=[]\n",
    "for item in f_reviews_list_all:\n",
    "    if item==[]:\n",
    "        f_reviews_all.append('NA')\n",
    "    else:\n",
    "        f_reviews_all.append(item[0])\n",
    "\n",
    "\n",
    "f_tags_all=[]\n",
    "for item in f_tags_list_all:\n",
    "    if item==[]:\n",
    "        f_tags_all.append('NA')\n",
    "    else:\n",
    "        f_tags_all.append(item)\n",
    "\n",
    "f_phone_all=[]\n",
    "for item in f_phone_list_all:\n",
    "    if item==[]:\n",
    "        f_phone_all.append('NA')\n",
    "    else:\n",
    "        f_phone_all.append(item[0])\n",
    "    \n",
    "        \n",
    "f_location_all=[]\n",
    "for item in f_location_list_all:\n",
    "    if item==[]:\n",
    "        f_location_all.append('NA')\n",
    "    else:\n",
    "        f_location_all.append(item[0])\n",
    "        \n",
    "\n",
    "f_address_all=[]\n",
    "for item in f_address_list_all:\n",
    "    if item==[]:\n",
    "        f_address_all.append('NA')\n",
    "    else:\n",
    "        f_address_all.append(item[0])\n",
    "        \n",
    "\n",
    "# splitting the tags list into multiple columns  \n",
    "# How many columns to do i need to create for category tags?\n",
    "# max(len(item) for item in f_tags_list_all)\n",
    "category_tag1=[]\n",
    "category_tag2=[]\n",
    "category_tag3=[]\n",
    "for item in f_tags_list_all:\n",
    "    if len(item)==0:\n",
    "        category_tag1.append('NA')\n",
    "        category_tag2.append('NA')\n",
    "        category_tag3.append('NA')\n",
    "    elif len(item)==1:\n",
    "        category_tag1.append(item[0])\n",
    "        category_tag2.append('NA')\n",
    "        category_tag3.append('NA')\n",
    "    elif len(item)==2:\n",
    "        category_tag1.append(item[0])\n",
    "        category_tag2.append(item[1])\n",
    "        category_tag3.append('NA')\n",
    "    elif len(item)==3:\n",
    "        category_tag1.append(item[0])\n",
    "        category_tag2.append(item[1])\n",
    "        category_tag3.append(item[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RestaurantName</th>\n",
       "      <th>PriceRange</th>\n",
       "      <th>ReviewCount</th>\n",
       "      <th>Category1</th>\n",
       "      <th>Category2</th>\n",
       "      <th>Category3</th>\n",
       "      <th>Address</th>\n",
       "      <th>Location</th>\n",
       "      <th>PhoneNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Mayfair Chippy</td>\n",
       "      <td>££</td>\n",
       "      <td>280</td>\n",
       "      <td>Fish &amp; Chips</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>14 North Audley Street</td>\n",
       "      <td>Mayfair</td>\n",
       "      <td>020 7741 2233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Dishoom</td>\n",
       "      <td>££</td>\n",
       "      <td>1842</td>\n",
       "      <td>Indian</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>12 Upper Saint Martin's Lane</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>020 7420 9320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Flat Iron</td>\n",
       "      <td>££</td>\n",
       "      <td>380</td>\n",
       "      <td>Steakhouses</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>17 Beak Street</td>\n",
       "      <td>Soho</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Ffiona’s Restaurant</td>\n",
       "      <td>££</td>\n",
       "      <td>270</td>\n",
       "      <td>British</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>51 Kensington Church Street</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>020 7937 4152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Restaurant Gordon Ramsay</td>\n",
       "      <td>££££</td>\n",
       "      <td>205</td>\n",
       "      <td>French</td>\n",
       "      <td>British</td>\n",
       "      <td>NA</td>\n",
       "      <td>68 Royal Hospital Road</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>020 7352 4441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>Battersea Pie</td>\n",
       "      <td>£</td>\n",
       "      <td>82</td>\n",
       "      <td>British</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>28 The Market</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>020 7240 9566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>Boro Bistro</td>\n",
       "      <td>££</td>\n",
       "      <td>40</td>\n",
       "      <td>Pubs</td>\n",
       "      <td>French</td>\n",
       "      <td>NA</td>\n",
       "      <td>6-10 Borough High Street</td>\n",
       "      <td>London Bridge</td>\n",
       "      <td>020 7378 0788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>Hawksmoor</td>\n",
       "      <td>£££</td>\n",
       "      <td>151</td>\n",
       "      <td>Seafood</td>\n",
       "      <td>Steakhouses</td>\n",
       "      <td>NA</td>\n",
       "      <td>5A Air Street</td>\n",
       "      <td>Mayfair</td>\n",
       "      <td>020 7406 3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>Grumbles</td>\n",
       "      <td>££</td>\n",
       "      <td>67</td>\n",
       "      <td>British</td>\n",
       "      <td>French</td>\n",
       "      <td>NA</td>\n",
       "      <td>35 Churton Street</td>\n",
       "      <td>Pimlico</td>\n",
       "      <td>020 7834 0149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>Casa do Frango</td>\n",
       "      <td>NA</td>\n",
       "      <td>4</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>Wine Bars</td>\n",
       "      <td>NA</td>\n",
       "      <td>32 Southwark Street</td>\n",
       "      <td>London Bridge</td>\n",
       "      <td>020 3972 2323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RestaurantName PriceRange ReviewCount     Category1  \\\n",
       "0          The Mayfair Chippy         ££         280  Fish & Chips   \n",
       "1                     Dishoom         ££        1842        Indian   \n",
       "2                   Flat Iron         ££         380   Steakhouses   \n",
       "3         Ffiona’s Restaurant         ££         270       British   \n",
       "4    Restaurant Gordon Ramsay       ££££         205        French   \n",
       "..                        ...        ...         ...           ...   \n",
       "235             Battersea Pie          £          82       British   \n",
       "236               Boro Bistro         ££          40          Pubs   \n",
       "237                 Hawksmoor        £££         151       Seafood   \n",
       "238                  Grumbles         ££          67       British   \n",
       "239            Casa do Frango         NA           4    Portuguese   \n",
       "\n",
       "       Category2 Category3                       Address       Location  \\\n",
       "0             NA        NA        14 North Audley Street        Mayfair   \n",
       "1             NA        NA  12 Upper Saint Martin's Lane  Covent Garden   \n",
       "2             NA        NA                17 Beak Street           Soho   \n",
       "3             NA        NA   51 Kensington Church Street     Kensington   \n",
       "4        British        NA        68 Royal Hospital Road        Chelsea   \n",
       "..           ...       ...                           ...            ...   \n",
       "235           NA        NA                 28 The Market  Covent Garden   \n",
       "236       French        NA      6-10 Borough High Street  London Bridge   \n",
       "237  Steakhouses        NA                 5A Air Street        Mayfair   \n",
       "238       French        NA             35 Churton Street        Pimlico   \n",
       "239    Wine Bars        NA           32 Southwark Street  London Bridge   \n",
       "\n",
       "       PhoneNumber  \n",
       "0    020 7741 2233  \n",
       "1    020 7420 9320  \n",
       "2               NA  \n",
       "3    020 7937 4152  \n",
       "4    020 7352 4441  \n",
       "..             ...  \n",
       "235  020 7240 9566  \n",
       "236  020 7378 0788  \n",
       "237  020 7406 3980  \n",
       "238  020 7834 0149  \n",
       "239  020 3972 2323  \n",
       "\n",
       "[240 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_data=pd.DataFrame({'RestaurantName':f_names_all,\n",
    "                        'PriceRange': f_price_all,\n",
    "                        'ReviewCount': f_reviews_all,\n",
    "                        #'Tags': f_tags_all,\n",
    "                        'Category1': category_tag1,\n",
    "                        'Category2': category_tag2,\n",
    "                        'Category3': category_tag3,\n",
    "                        'Address': f_address_all,\n",
    "                        'Location': f_location_all,\n",
    "                        'PhoneNumber': f_phone_all\n",
    "                       })\n",
    "yelp_data\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
